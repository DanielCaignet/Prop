{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvU46iLifbGtxvMJhTVm/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielCaignet/Prop/blob/main/VP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Python translation of ProyectoFinal_TEC_Daniel_Caignet.m\n",
        "\n",
        "This script loads the vehicles dataset, performs preprocessing,\n",
        "trains a 1D CNN using cross validation and evaluates the final model.\n",
        "It follows the logic of the original MATLAB code.\n",
        "\n",
        "Note: The vehicles_.mat file must contain a variable ``vehicles`` which is an\n",
        "array of structs with fields ``high`` and ``power``.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy.interpolate import interp1d\n",
        "import pywt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Utility functions\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def map_values(x, in_min, in_max, out_min, out_max):\n",
        "    \"\"\"Replicates the map function from the MATLAB code.\"\"\"\n",
        "    x_map = (x - in_min) * ((out_max - out_min) / (in_max - in_min)) + out_min\n",
        "    return np.clip(x_map, out_min, out_max)\n",
        "\n",
        "\n",
        "def inter2mean(x, n_mean):\n",
        "    \"\"\"Interpolate a 1‑D sequence to ``n_mean`` points using cubic splines.\"\"\"\n",
        "    n = len(x)\n",
        "    x_old = np.linspace(1, n, n)\n",
        "    x_new = np.linspace(1, n, n_mean)\n",
        "    f = interp1d(x_old, x, kind=\"cubic\")\n",
        "    return f(x_new)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Load data and split train/test\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def load_data(path=\"vehicles_.mat\", test_ratio=0.2, seed=42):\n",
        "    data = loadmat(path)\n",
        "    vehicles = data[\"vehicles\"].squeeze()\n",
        "\n",
        "    n = vehicles.size\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(n)\n",
        "    n_test = int(round(test_ratio * n))\n",
        "    idx_test = idx[:n_test]\n",
        "    idx_train = idx[n_test:]\n",
        "\n",
        "    v_train = vehicles[idx_train]\n",
        "    v_test = vehicles[idx_test]\n",
        "\n",
        "    return v_train, v_test, idx_train, idx_test\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Preprocessing\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def prepare_data(v_train, v_test):\n",
        "    # Labels\n",
        "    h_train = np.array([v[\"high\"].item() for v in v_train])\n",
        "    h_test = np.array([v[\"high\"].item() for v in v_test])\n",
        "\n",
        "    # Discretize into 3 classes\n",
        "    edges = np.linspace(-1, 1, 4)\n",
        "    h_min = h_train.min()\n",
        "    h_max = h_train.max()\n",
        "\n",
        "    h_tn = map_values(h_train, h_min, h_max, 0.1, 1)\n",
        "    y_log = np.log10(h_tn)\n",
        "    y_min = y_log.min()\n",
        "    y_max = y_log.max()\n",
        "    y_norm_train = map_values(y_log, y_min, y_max, -1, 1)\n",
        "    clases_tr = np.digitize(y_norm_train, edges, right=True)\n",
        "\n",
        "    h_tn2 = map_values(h_test, h_min, h_max, 0.1, 1)\n",
        "    y_log2 = np.log10(h_tn2)\n",
        "    y_norm_test = map_values(y_log2, y_min, y_max, -1, 1)\n",
        "    clases_te = np.digitize(y_norm_test, edges, right=True)\n",
        "\n",
        "    y_train = clases_tr.astype(int) - 1  # classes 0..2\n",
        "    y_test = clases_te.astype(int) - 1\n",
        "\n",
        "    # Interpolate series to same length\n",
        "    n_tr = len(v_train)\n",
        "    lengths = [len(v[\"power\"].squeeze()) for v in v_train]\n",
        "    T = round(np.mean(lengths) / 2)\n",
        "\n",
        "    pmat_train = np.zeros((n_tr, T))\n",
        "    for i, v in enumerate(v_train):\n",
        "        pmat_train[i] = inter2mean(v[\"power\"].squeeze(), T)\n",
        "\n",
        "    n_ts = len(v_test)\n",
        "    pmat_test = np.zeros((n_ts, T))\n",
        "    for i, v in enumerate(v_test):\n",
        "        pmat_test[i] = inter2mean(v[\"power\"].squeeze(), T)\n",
        "    # MODWT + global normalization\n",
        "    wname = \"db1\"\n",
        "    n_levels = 4\n",
        "    n_feat = n_levels + 1\n",
        "    pad_len = (-T) % (2**n_levels)\n",
        "\n",
        "    wmat_train = np.zeros((n_tr, n_feat, T))\n",
        "    for i in range(n_tr):\n",
        "        if hasattr(pywt, 'modwt'):\n",
        "            coeffs = pywt.modwt(pmat_train[i], wname, level=n_levels)\n",
        "            coeffs = [coeffs[-1]] + coeffs[-2::-1]\n",
        "        else:\n",
        "            pad = np.pad(pmat_train[i], (0, pad_len), mode='symmetric') if pad_len else pmat_train[i]\n",
        "            swt_coeffs = pywt.swt(pad, wname, level=n_levels)\n",
        "            coeffs = [swt_coeffs[-1][0][:T]] + [c[1][:T] for c in swt_coeffs[::-1]]\n",
        "        wmat_train[i] = np.stack(coeffs, axis=0)\n",
        "\n",
        "    w_min = wmat_train.min(axis=(0, 2))\n",
        "    w_max = wmat_train.max(axis=(0, 2))\n",
        "    w3d_train = 2 * (wmat_train - w_min) / (w_max - w_min) - 1\n",
        "\n",
        "    x_train_seq = [w3d_train[i].T for i in range(n_tr)]\n",
        "\n",
        "    wmat_test = np.zeros((n_ts, n_feat, T))\n",
        "    for i in range(n_ts):\n",
        "        if hasattr(pywt, 'modwt'):\n",
        "            coeffs = pywt.modwt(pmat_test[i], wname, level=n_levels)\n",
        "            coeffs = [coeffs[-1]] + coeffs[-2::-1]\n",
        "        else:\n",
        "            pad = np.pad(pmat_test[i], (0, pad_len), mode='symmetric') if pad_len else pmat_test[i]\n",
        "            swt_coeffs = pywt.swt(pad, wname, level=n_levels)\n",
        "            coeffs = [swt_coeffs[-1][0][:T]] + [c[1][:T] for c in swt_coeffs[::-1]]\n",
        "        wmat_test[i] = np.stack(coeffs, axis=0)\n",
        "\n",
        "    w3d_test = 2 * (wmat_test - w_min) / (w_max - w_min) - 1\n",
        "    x_test_seq = [w3d_test[i].T for i in range(n_ts)]\n",
        "\n",
        "    return x_train_seq, y_train, x_test_seq, y_test, T, n_feat\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Build model\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class CNN1D(nn.Module):\n",
        "    \"\"\"Simple 1D CNN matching the original architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, n_feat):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(n_feat, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(32, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def build_model(n_feat):\n",
        "    \"\"\"Instantiate the CNN model.\"\"\"\n",
        "    return CNN1D(n_feat)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Training with K-Fold cross validation\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def train_with_kfold(x_train_seq, y_train, T, n_feat, k=5, epochs=30, batch_size=16):\n",
        "    \"\"\"Train the CNN using k-fold cross validation.\"\"\"\n",
        "    y_train = np.array(y_train)\n",
        "    tbl = np.bincount(y_train)\n",
        "    weight = torch.tensor([tbl.sum() / tbl[i] for i in range(len(tbl))], dtype=torch.float32)\n",
        "\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    acc_folds = []\n",
        "    prec_folds = []\n",
        "    rec_folds = []\n",
        "    f1_folds = []\n",
        "\n",
        "    for train_index, val_index in kf.split(x_train_seq):\n",
        "        x_tr = torch.tensor([x_train_seq[i].T for i in train_index], dtype=torch.float32)\n",
        "        y_tr = torch.tensor(y_train[train_index], dtype=torch.long)\n",
        "        x_val = torch.tensor([x_train_seq[i].T for i in val_index], dtype=torch.float32)\n",
        "        y_val = torch.tensor(y_train[val_index], dtype=torch.long)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(x_tr, y_tr), batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=batch_size)\n",
        "\n",
        "        model = build_model(n_feat)\n",
        "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        for _ in range(epochs):\n",
        "            model.train()\n",
        "            for xb, yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in val_loader:\n",
        "                preds.append(model(xb).argmax(dim=1).cpu().numpy())\n",
        "        y_pred = np.concatenate(preds)\n",
        "        y_true = y_val.numpy()\n",
        "\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "        acc = (y_pred == y_true).mean()\n",
        "\n",
        "        acc_folds.append(acc)\n",
        "        prec_folds.append(prec)\n",
        "        rec_folds.append(rec)\n",
        "        f1_folds.append(f1)\n",
        "\n",
        "        print(f\"Fold {len(acc_folds)} — Acc: {acc*100:.2f}%, \"\n",
        "              f\"Prec: {prec:.3f}, Rec: {rec:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "    print(f\"CV mean Acc: {np.mean(acc_folds)*100:.2f}% ± {np.std(acc_folds)*100:.2f}%\")\n",
        "    print(f\"CV mean Prec: {np.mean(prec_folds):.3f} ± {np.std(prec_folds):.3f}\")\n",
        "    print(f\"CV mean Rec:  {np.mean(rec_folds):.3f} ± {np.std(rec_folds):.3f}\")\n",
        "    print(f\"CV mean F1:   {np.mean(f1_folds):.3f} ± {np.std(f1_folds):.3f}\")\n",
        "\n",
        "    # Train final model on all data\n",
        "    x_all = torch.tensor([x.T for x in x_train_seq], dtype=torch.float32)\n",
        "    y_all = torch.tensor(y_train, dtype=torch.long)\n",
        "    loader = DataLoader(TensorDataset(x_all, y_all), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = build_model(n_feat)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Evaluation\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def evaluate(model, x_test_seq, y_test, batch_size=16):\n",
        "    \"\"\"Evaluate the trained model on the test set.\"\"\"\n",
        "    x_ts = torch.tensor([x.T for x in x_test_seq], dtype=torch.float32)\n",
        "    y_ts = torch.tensor(y_test, dtype=torch.long)\n",
        "    loader = DataLoader(TensorDataset(x_ts, y_ts), batch_size=batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            preds.append(model(xb).argmax(dim=1).cpu().numpy())\n",
        "    y_pred = np.concatenate(preds)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    acc = (y_pred == y_test).mean()\n",
        "    print(f\"Test — Acc: {acc*100:.2f}%, Prec: {prec:.3f}, Rec: {rec:.3f}, F1: {f1:.3f}\")\n",
        "    print(cm)\n",
        "    idx = np.random.choice(len(y_test), 3, replace=False)\n",
        "    print(\" Índice original | Altura real | Clase Real | Clase Predicha\")\n",
        "    for i in idx:\n",
        "        print(f\" {i:14d}  | {y_test[i]:11d} | {y_test[i]:9d} | {y_pred[i]:9d}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main entry point\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "v_train, v_test, idx_train, idx_test = load_data()\n",
        "x_train_seq, y_train, x_test_seq, y_test, T, n_feat = prepare_data(v_train, v_test)\n",
        "model = train_with_kfold(x_train_seq, y_train, T, n_feat)\n",
        "evaluate(model, x_test_seq, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "XFjJATEZi_HY",
        "outputId": "63d265fa-5fed-472f-cf56-4eb51fb189ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (423,5,1084) (5,) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-2723760859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0mv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m \u001b[0mx_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-2723760859.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(v_train, v_test)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mw_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmat_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mw_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmat_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mw3d_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwmat_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_min\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_min\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mx_train_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw3d_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (423,5,1084) (5,) "
          ]
        }
      ]
    }
  ]
}